{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Theory\n",
    "\n",
    "The combined model is trained with the triple criterion*:\n",
    "\n",
    "$\\mathcal{L} = \\mathcal{L}_{prior} + \\mathcal{L}_{llike}^{Dis_l} + \\mathcal{L}_{GAN}$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathcal{L}_{prior} = D_{KL}(q(z|x) \\Vert p(z))$\n",
    "\n",
    "$\\mathcal{L}_{GAN} = \\log(Dis(x)) + \\log(1-Dis(Dec(z))) + \\log(1-Dis(Dec(Enc(x))))$            \n",
    "\n",
    "$\\mathcal{L}_{llike}^{Dis_l} = -\\mathbb{E}_{q(z|x)}[\\log p(Dis_l (x)|z)]$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "$x$ is a training sample and $z \\sim p(z)$\n",
    "(Dis = discriminator; Gen = Decoder)\n",
    "\n",
    "In addition, for $\\mathcal{L}_{GAN}$, $x = Dec(z)$ with $z \\sim p(z)$\n",
    "\n",
    "$\\mathcal{L}_{GAN}$ is the style error;\n",
    "Reconstruction error is the content error\n",
    "\n",
    "---\n",
    "\\*  The authors do not explicitly do this, see the gradient updates section below. (They seem to use this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mathcal{L}_{llike}^{Dis_l}$\n",
    "\n",
    "$Dis_l(x)$ is the hidden representation of the $l$ th layer of the discriminator\n",
    "\n",
    "Uses a Gaussian observation model for $Dis_l(x)$\n",
    "- Mean: $Dis_l(\\tilde x)$ where $\\tilde x \\sim Dec(z)$ is the sample from the decoder of $x$.\n",
    "- Identity covariance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Practice\n",
    "\n",
    "- Do not update all network parameters wrt. the combined loss\n",
    "    * The Discriminator should not minimise $\\mathcal{L}_{llike}^{Dis_l}$ (collapses discriminator to 0)\n",
    "    * Do not backpropagate the error signal from $\\mathcal{L}_{GAN}$ to the Encoder\n",
    "\n",
    "- Use a parameter $\\gamma$ to weight the ability to reconstruct vs. fooling the discriminator\n",
    "    * not applied to the entire model\n",
    "    * weighting only applied when updating the parameters of the decoder.\n",
    "    * $\\theta_{Dec} \\stackrel{+}\\leftarrow - \\nabla_{\\theta_{Dec}} (\\gamma \\mathcal{L}_{llike}^{Dis_l} - \\mathcal{L}_{GAN})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating gradients\n",
    "\n",
    "1. $\\theta_{Enc} \\stackrel{+}\\leftarrow - \\nabla_{\\theta_{Enc}} (\\mathcal{L}_{prior} + \\mathcal{L}_{llike}^{Dis_l})$\n",
    "2. $\\theta_{Dec} \\stackrel{+}\\leftarrow - \\nabla_{\\theta_{Dec}} (\\gamma \\mathcal{L}_{llike}^{Dis_l} - \\mathcal{L}_{GAN})$\n",
    "3. $\\theta_{Dis} \\stackrel{+}\\leftarrow - \\nabla_{\\theta_{Dis}} \\mathcal{L}_{GAN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. After encoder runs, calculate $\\mathcal{L}_{prior}$\n",
    "\n",
    "2. After decoder runs, (and maybe the disc), calculate the layer-wise disc loss,$\\mathcal{L}_{llike}^{Dis_l}$\n",
    "\n",
    "3. After running the random sample through the decoder, calculate $\\mathcal{L}_{GAN}$\n",
    "\n",
    "4. Update the parameters using the gradients (see previous section)\n",
    "\n",
    "5. Note the algorithm says \"until deadline\" (we will need to define the deadline)\n",
    "\n",
    "\n",
    "**(See Figure and Algorithm on top of page 3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
