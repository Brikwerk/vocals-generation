{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_vc import Generator\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from synthesis import build_model, wavegen\n",
    "from hparams import hparams\n",
    "from wavenet_vocoder import WaveNet\n",
    "from wavenet_vocoder.util import is_mulaw_quantize, is_mulaw, is_raw, is_scalar_input\n",
    "from tqdm import tqdm\n",
    "import audio\n",
    "from nnmnkwii import preprocessing as P\n",
    "import numpy as np\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Generator(160, 0, 512, 16)\n",
    "g.load_state_dict(torch.load('model_latest.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 880, 320)\n",
    "plt.imshow(x.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_outputs = g.decoder(x)\n",
    "                \n",
    "mel_outputs_postnet = g.postnet(mel_outputs.transpose(2,1))\n",
    "mel_outputs_postnet = mel_outputs + mel_outputs_postnet.transpose(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mel_outputs_postnet.squeeze(0).squeeze(0).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeded Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('example_vocals-feats.npy')\n",
    "x = torch.from_numpy(x)\n",
    "x = x[:720, :].unsqueeze(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_outputs, mel_outputs_postnet, encoder_output = g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_outputs_postnet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    if is_mulaw_quantize(hparams.input_type):\n",
    "        if hparams.out_channels != hparams.quantize_channels:\n",
    "            raise RuntimeError(\n",
    "                \"out_channels must equal to quantize_chennels if input_type is 'mulaw-quantize'\")\n",
    "    if hparams.upsample_conditional_features and hparams.cin_channels < 0:\n",
    "        s = \"Upsample conv layers were specified while local conditioning disabled. \"\n",
    "        s += \"Notice that upsample conv layers will never be used.\"\n",
    "        print(s)\n",
    "\n",
    "    upsample_params = hparams.upsample_params\n",
    "    upsample_params[\"cin_channels\"] = hparams.cin_channels\n",
    "    upsample_params[\"cin_pad\"] = hparams.cin_pad\n",
    "    model = WaveNet(\n",
    "        out_channels=hparams.out_channels,\n",
    "        layers=hparams.layers,\n",
    "        stacks=hparams.stacks,\n",
    "        residual_channels=hparams.residual_channels,\n",
    "        gate_channels=hparams.gate_channels,\n",
    "        skip_out_channels=hparams.skip_out_channels,\n",
    "        cin_channels=hparams.cin_channels,\n",
    "        gin_channels=hparams.gin_channels,\n",
    "        n_speakers=hparams.n_speakers,\n",
    "        dropout=hparams.dropout,\n",
    "        kernel_size=hparams.kernel_size,\n",
    "        cin_pad=hparams.cin_pad,\n",
    "        upsample_conditional_features=hparams.upsample_conditional_features,\n",
    "        upsample_params=upsample_params,\n",
    "        scalar_input=is_scalar_input(hparams.input_type),\n",
    "        output_distribution=hparams.output_distribution,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def batch_wavegen(model, c=None, g=None, fast=True, tqdm=tqdm):\n",
    "    assert c is not None\n",
    "    B = c.shape[0]\n",
    "    model.eval()\n",
    "    if fast:\n",
    "        model.make_generation_fast_()\n",
    "\n",
    "    # Transform data to GPU\n",
    "    g = None if g is None else g.to(device)\n",
    "    c = None if c is None else c.to(device)\n",
    "\n",
    "    if hparams.upsample_conditional_features:\n",
    "        length = (c.shape[-1] - hparams.cin_pad * 2) * audio.get_hop_size()\n",
    "    else:\n",
    "        # already dupulicated\n",
    "        length = c.shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat = model.incremental_forward(\n",
    "            c=c, g=g, T=length, tqdm=tqdm, softmax=True, quantize=True,\n",
    "            log_scale_min=hparams.log_scale_min)\n",
    "\n",
    "    if is_mulaw_quantize(hparams.input_type):\n",
    "        # needs to be float since mulaw_inv returns in range of [-1, 1]\n",
    "        y_hat = y_hat.max(1)[1].view(B, -1).float().cpu().data.numpy()\n",
    "        for i in range(B):\n",
    "            y_hat[i] = P.inv_mulaw_quantize(y_hat[i], hparams.quantize_channels - 1)\n",
    "    elif is_mulaw(hparams.input_type):\n",
    "        y_hat = y_hat.view(B, -1).cpu().data.numpy()\n",
    "        for i in range(B):\n",
    "            y_hat[i] = P.inv_mulaw(y_hat[i], hparams.quantize_channels - 1)\n",
    "    else:\n",
    "        y_hat = y_hat.view(B, -1).cpu().data.numpy()\n",
    "\n",
    "    if hparams.postprocess is not None and hparams.postprocess not in [\"\", \"none\"]:\n",
    "        for i in range(B):\n",
    "            y_hat[i] = getattr(audio, hparams.postprocess)(y_hat[i])\n",
    "\n",
    "    if hparams.global_gain_scale > 0:\n",
    "        for i in range(B):\n",
    "            y_hat[i] /= hparams.global_gain_scale\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def to_int16(x):\n",
    "    if x.dtype == np.int16:\n",
    "        return x\n",
    "    assert x.dtype == np.float32\n",
    "    assert x.min() >= -1 and x.max() <= 1.0\n",
    "    return (x * 32767).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = build_model().to(device)\n",
    "checkpoint = torch.load(\"/wavenet_vocoder/checkpoints/checkpoint_latest_ema.pth\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = mel_outputs_postnet.squeeze(0).detach()\n",
    "\n",
    "# Split c into chunks across the 0th dimension\n",
    "length = c.shape[0]\n",
    "c = c.T\n",
    "print(c.shape)\n",
    "c_chunks = c.reshape(80, length//20, 20)\n",
    "c_chunks = c_chunks.permute(1, 0, 2)\n",
    "c = c_chunks\n",
    "print(c.shape)\n",
    "\n",
    "# # Resize c to 1, 80, 866\n",
    "# print(c.shape)\n",
    "# c = TF.resize(c, (80, 866))\n",
    "# c = c[:, :, :50]\n",
    "# print(c.shape)\n",
    "\n",
    "# Generate\n",
    "y_hats = batch_wavegen(model, c=c, g=None, fast=True, tqdm=tqdm)\n",
    "y_hats = torch.from_numpy(y_hats).flatten().unsqueeze(0).numpy()\n",
    "\n",
    "gen = y_hats[0]\n",
    "gen = np.clip(gen, -1.0, 1.0)\n",
    "wavfile.write('test.wav', hparams.sample_rate, to_int16(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
