{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "\n",
    "from model_vc import Generator, GeneratorV2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from synthesis import build_model, wavegen\n",
    "from hparams import hparams\n",
    "from wavenet_vocoder import WaveNet\n",
    "from wavenet_vocoder.util import is_mulaw_quantize, is_mulaw, is_raw, is_scalar_input\n",
    "from tqdm import tqdm\n",
    "import audio\n",
    "from nnmnkwii import preprocessing as P\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from data_loader import SpecsCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accompaniment Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_accom = Generator(160, 0, 512, 20)\n",
    "g_accom.load_state_dict(torch.load('model_latest_accom.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpecsCombined('~/Data/segments_combined', len_crop=860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accom_spec, vocals_spec = dataset[500]\n",
    "accom_spec = accom_spec.unsqueeze(0)\n",
    "vocals_spec = vocals_spec.unsqueeze(0)\n",
    "print(accom_spec.shape, vocals_spec.shape)\n",
    "_, vocals_spec_2 = dataset[2]\n",
    "vocals_spec_2 = vocals_spec_2.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accompaniment Latent Vector Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accom_vec = g_accom(accom_spec, return_encoder_output=True)\n",
    "accom_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 860, 80)\n",
    "# x = torch.sin(x)\n",
    "plt.imshow(x.squeeze(0))\n",
    "# x_noise = torch.FloatTensor(1, 860, 320).uniform_(-0.06, 0.06)\n",
    "# plt.imshow(x_noise.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('example_vocals-feats.npy')\n",
    "x = torch.from_numpy(x)\n",
    "x = x[:860, :].unsqueeze(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocals Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_vocals = GeneratorV2(160, 0, 512, 20, 860, 128)\n",
    "g_vocals.load_state_dict(torch.load('model_lowest_val_vae.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Latent Vector Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_vec = g_vocals.cond_proj(accom_vec.flatten(start_dim=1))\n",
    "latent_vec = torch.cat((torch.rand(1, 128), condition_vec), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeded Latent Vector Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_vec = g_vocals.cond_proj(accom_vec.flatten(start_dim=1))\n",
    "\n",
    "vocal_vec_1 = g_vocals.vocals_proj(g_vocals(vocals_spec, return_encoder_output=True).flatten(start_dim=1))\n",
    "vocal_vec_2 = g_vocals.vocals_proj(g_vocals(vocals_spec_2, return_encoder_output=True).flatten(start_dim=1))\n",
    "\n",
    "# # Take the average of the two\n",
    "# vocal_vec = (vocal_vec_1 + vocal_vec_2) / 2\n",
    "\n",
    "# vocal_vec = (vocal_vec_1 * 0.5) + (vocal_vec_2 * 0.5)\n",
    "\n",
    "# vocal_vec = vocal_vec_1 + (vocal_vec_2 * 0.5)\n",
    "\n",
    "latent_vec = torch.cat((vocal_vec_1, condition_vec), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparameterization trick\n",
    "mu = g_vocals.mu_fc(latent_vec)\n",
    "logvar = g_vocals.logvar_fc(latent_vec)\n",
    "std = torch.exp(logvar / 2)\n",
    "q = torch.distributions.Normal(mu, std)\n",
    "z = q.rsample()\n",
    "\n",
    "encoder_outputs = g_vocals.latent_proj(z)\n",
    "\n",
    "encoder_outputs = encoder_outputs.reshape(1, 860, 320)\n",
    "\n",
    "plt.imshow(vocals_spec.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_outputs = g_vocals.decoder(encoder_outputs)\n",
    "                \n",
    "mel_outputs_postnet = g_vocals.postnet(mel_outputs.transpose(2,1))\n",
    "mel_outputs_postnet = mel_outputs + mel_outputs_postnet.transpose(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mel_outputs_postnet.squeeze(0).squeeze(0).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    if is_mulaw_quantize(hparams.input_type):\n",
    "        if hparams.out_channels != hparams.quantize_channels:\n",
    "            raise RuntimeError(\n",
    "                \"out_channels must equal to quantize_chennels if input_type is 'mulaw-quantize'\")\n",
    "    if hparams.upsample_conditional_features and hparams.cin_channels < 0:\n",
    "        s = \"Upsample conv layers were specified while local conditioning disabled. \"\n",
    "        s += \"Notice that upsample conv layers will never be used.\"\n",
    "        print(s)\n",
    "\n",
    "    upsample_params = hparams.upsample_params\n",
    "    upsample_params[\"cin_channels\"] = hparams.cin_channels\n",
    "    upsample_params[\"cin_pad\"] = hparams.cin_pad\n",
    "    model = WaveNet(\n",
    "        out_channels=hparams.out_channels,\n",
    "        layers=hparams.layers,\n",
    "        stacks=hparams.stacks,\n",
    "        residual_channels=hparams.residual_channels,\n",
    "        gate_channels=hparams.gate_channels,\n",
    "        skip_out_channels=hparams.skip_out_channels,\n",
    "        cin_channels=hparams.cin_channels,\n",
    "        gin_channels=hparams.gin_channels,\n",
    "        n_speakers=hparams.n_speakers,\n",
    "        dropout=hparams.dropout,\n",
    "        kernel_size=hparams.kernel_size,\n",
    "        cin_pad=hparams.cin_pad,\n",
    "        upsample_conditional_features=hparams.upsample_conditional_features,\n",
    "        upsample_params=upsample_params,\n",
    "        scalar_input=is_scalar_input(hparams.input_type),\n",
    "        output_distribution=hparams.output_distribution,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def batch_wavegen(model, c=None, g=None, fast=True, tqdm=tqdm):\n",
    "    assert c is not None\n",
    "    B = c.shape[0]\n",
    "    model.eval()\n",
    "    if fast:\n",
    "        model.make_generation_fast_()\n",
    "\n",
    "    # Transform data to GPU\n",
    "    g = None if g is None else g.to(device)\n",
    "    c = None if c is None else c.to(device)\n",
    "\n",
    "    if hparams.upsample_conditional_features:\n",
    "        length = (c.shape[-1] - hparams.cin_pad * 2) * audio.get_hop_size()\n",
    "    else:\n",
    "        # already dupulicated\n",
    "        length = c.shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat = model.incremental_forward(\n",
    "            c=c, g=g, T=length, tqdm=tqdm, softmax=True, quantize=True,\n",
    "            log_scale_min=hparams.log_scale_min)\n",
    "\n",
    "    if is_mulaw_quantize(hparams.input_type):\n",
    "        # needs to be float since mulaw_inv returns in range of [-1, 1]\n",
    "        y_hat = y_hat.max(1)[1].view(B, -1).float().cpu().data.numpy()\n",
    "        for i in range(B):\n",
    "            y_hat[i] = P.inv_mulaw_quantize(y_hat[i], hparams.quantize_channels - 1)\n",
    "    elif is_mulaw(hparams.input_type):\n",
    "        y_hat = y_hat.view(B, -1).cpu().data.numpy()\n",
    "        for i in range(B):\n",
    "            y_hat[i] = P.inv_mulaw(y_hat[i], hparams.quantize_channels - 1)\n",
    "    else:\n",
    "        y_hat = y_hat.view(B, -1).cpu().data.numpy()\n",
    "\n",
    "    if hparams.postprocess is not None and hparams.postprocess not in [\"\", \"none\"]:\n",
    "        for i in range(B):\n",
    "            y_hat[i] = getattr(audio, hparams.postprocess)(y_hat[i])\n",
    "\n",
    "    if hparams.global_gain_scale > 0:\n",
    "        for i in range(B):\n",
    "            y_hat[i] /= hparams.global_gain_scale\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def to_int16(x):\n",
    "    if x.dtype == np.int16:\n",
    "        return x\n",
    "    assert x.dtype == np.float32\n",
    "    assert x.min() >= -1 and x.max() <= 1.0\n",
    "    return (x * 32767).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = build_model().to(device)\n",
    "checkpoint = torch.load(\"/wavenet_vocoder/checkpoints/checkpoint_latest_ema.pth\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = (mel_outputs_postnet/2) + (accom_spec/2)\n",
    "# c = outputs.squeeze(0).detach()\n",
    "\n",
    "num_chunks = 20\n",
    "\n",
    "# Original vocals\n",
    "# c = vocals_spec.squeeze(0).detach()\n",
    "# Vocal output\n",
    "c = mel_outputs_postnet.squeeze(0).detach()\n",
    "# Accom output\n",
    "# c = accom_spec.squeeze(0).detach()\n",
    "\n",
    "# Split c into chunks across the 0th dimension\n",
    "length = c.shape[0]\n",
    "c = c.T\n",
    "c_chunks = c.reshape(80, length//num_chunks, num_chunks)\n",
    "c_chunks = c_chunks.permute(1, 0, 2)\n",
    "c = c_chunks\n",
    "\n",
    "# # Resize c to 1, 80, 866\n",
    "# print(c.shape)\n",
    "# c = TF.resize(c, (80, 866))\n",
    "# c = c[:, :, :50]\n",
    "# print(c.shape)\n",
    "\n",
    "# Generate\n",
    "y_hats = batch_wavegen(model, c=c, g=None, fast=True, tqdm=tqdm)\n",
    "y_hats = torch.from_numpy(y_hats).flatten().unsqueeze(0).numpy()\n",
    "\n",
    "gen = y_hats[0]\n",
    "gen = np.clip(gen, -1.0, 1.0)\n",
    "wavfile.write('test.wav', hparams.sample_rate, to_int16(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocals models\n",
    "# torch.save(g_vocals.state_dict(), './model_v3_7k.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE Visualization of Song Distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "g_accom.to(device)\n",
    "g_vocals.to(device)\n",
    "\n",
    "vecs = []\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    accom_spec, vocals_spec = dataset[i]\n",
    "    accom_spec = accom_spec.unsqueeze(0).to(device)\n",
    "    vocals_spec = vocals_spec.unsqueeze(0).to(device)\n",
    "\n",
    "    accom_vec = g_accom(accom_spec, return_encoder_output=True)\n",
    "    condition_vec = g_vocals.cond_proj(accom_vec.flatten(start_dim=1))\n",
    "    vocal_vec = g_vocals.vocals_proj(g_vocals(vocals_spec, return_encoder_output=True).flatten(start_dim=1))\n",
    "\n",
    "    latent_vec = torch.cat((vocal_vec, condition_vec), dim=-1)\n",
    "\n",
    "    vecs.append(latent_vec.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels from the file list in the dataset\n",
    "file_list = dataset.files\n",
    "name_to_label = {}\n",
    "labels = []\n",
    "for file in file_list:\n",
    "    name = file.split('/')[-1].split('_')[0]\n",
    "\n",
    "    if name not in name_to_label:\n",
    "        name_to_label[name] = len(name_to_label)\n",
    "    \n",
    "    labels.append(name_to_label[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack numpy list into a single numpy array\n",
    "vec_stack = np.vstack(vecs)\n",
    "# Get a list of numbers from 0 - 178 as a numpy array\n",
    "# num_list = np.arange(0, 178)\n",
    "print(\"Number of songs:\", len(np.unique(labels)))\n",
    "print(\"Number of labels:\", len(labels))\n",
    "print(\"Number of vectors:\", len(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter vectors for first 10 songs\n",
    "num_songs = 80\n",
    "offset = 70\n",
    "filtered_vecs = []\n",
    "filtered_labels = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] < num_songs and labels[i] >= offset:\n",
    "        filtered_vecs.append(vec_stack[i])\n",
    "        filtered_labels.append(labels[i])\n",
    "\n",
    "filtered_vec_stack = np.vstack(filtered_vecs)\n",
    "filtered_labels = np.array(filtered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "tsne = TSNE(n_components, learning_rate='auto', init='pca')\n",
    "tsne_result = tsne.fit_transform(filtered_vec_stack)\n",
    "tsne_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': filtered_labels})\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_result_df, ax=ax,s=120, palette=\"tab10\")\n",
    "lim = (tsne_result.min()-5, tsne_result.max()+5)\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Album-based Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_meta = pd.read_csv('song_meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song, artist, writer, album, year, ref = song_meta.iloc[0]\n",
    "\n",
    "# Match song title to album\n",
    "name_to_album = {}\n",
    "for i in range(len(name_to_label.keys())):\n",
    "    song_name = list(name_to_label.keys())[i].lower()\n",
    "    # Loop through all songs in song_meta and store the album name\n",
    "    # if a song name matches the song name in the dataset\n",
    "    for j in range(len(song_meta)):\n",
    "        song, artist, writer, album, year, ref = song_meta.iloc[j]\n",
    "        # if album not in ('1989', 'Taylor Swift'):\n",
    "        #     continue\n",
    "        song = song.lower().replace('\"', '')\n",
    "        \n",
    "        album = album.replace('(Deluxe edition)', '').split(' ')[0]\n",
    "\n",
    "        if song in song_name:\n",
    "            name_to_album[song_name] = album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_labels = []\n",
    "album_vecs = []\n",
    "for i in range(len(file_list)):\n",
    "    file = file_list[i]\n",
    "    vec = vecs[i]\n",
    "\n",
    "    name = file.split('/')[-1].split('_')[0]\n",
    "    name = name.lower()\n",
    "    \n",
    "    if name in name_to_album:\n",
    "        album_labels.append(name_to_album[name])\n",
    "        album_vecs.append(vec)\n",
    "\n",
    "album_vecs = np.vstack(album_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "tsne = TSNE(n_components, learning_rate='auto', init='pca')\n",
    "tsne_result = tsne.fit_transform(album_vecs)\n",
    "tsne_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': album_labels})\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_result_df, ax=ax,s=120)\n",
    "lim = (tsne_result.min()-5, tsne_result.max()+5)\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Vector Song Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_mean_vec = {}\n",
    "for file in file_list:\n",
    "    name = file.split('/')[-1].split('_')[0]\n",
    "\n",
    "    if name not in name_to_mean_vec:\n",
    "        name_to_mean_vec[name] = []\n",
    "    \n",
    "    name_to_mean_vec[name].append(vecs[i])\n",
    "\n",
    "mean_vec_labels = []\n",
    "mean_vecs = []\n",
    "for name in name_to_mean_vec:\n",
    "    mean_vec_labels.append(name)\n",
    "    mean_vecs.append(np.mean(name_to_mean_vec[name], axis=0))\n",
    "mean_vecs = np.vstack(mean_vecs)\n",
    "\n",
    "mean_vecs.shape, len(mean_vec_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter vectors for first 10 songs\n",
    "num_songs = 170\n",
    "filtered_mean_vec_labels = mean_vec_labels[:num_songs]\n",
    "filtered_mean_vecs = mean_vecs[:num_songs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "tsne = TSNE(n_components, learning_rate=200, init='pca')\n",
    "tsne_result = tsne.fit_transform(filtered_mean_vecs)\n",
    "tsne_result.shape\n",
    "\n",
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': filtered_mean_vec_labels})\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_result_df, ax=ax,s=120)\n",
    "lim = (tsne_result.min()-100, tsne_result.max()+100)\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tsne_result)):\n",
    "    print(i, tsne_result[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_id = 34\n",
    "print(filtered_mean_vec_labels[start_id], filtered_mean_vec_labels[start_id + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Vectors Labelled by Album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_mean_vec_labels = []\n",
    "album_mean_vecs = []\n",
    "for i in range(len(mean_vec_labels)):\n",
    "    name = mean_vec_labels[i]\n",
    "    name = name.lower()\n",
    "    vec = mean_vecs[i]\n",
    "\n",
    "    if name in name_to_album:\n",
    "        album_mean_vec_labels.append(name_to_album[name])\n",
    "        album_mean_vecs.append(vec)\n",
    "\n",
    "album_mean_vecs = np.vstack(album_mean_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "tsne = TSNE(n_components, learning_rate=200, init='pca')\n",
    "tsne_result = tsne.fit_transform(album_mean_vecs)\n",
    "print(tsne_result.shape)\n",
    "\n",
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': album_mean_vec_labels})\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_result_df, ax=ax,s=120, palette=\"tab10\")\n",
    "lim = (tsne_result.min()-250, tsne_result.max()+250)\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_mean_vecs.shape, len(album_mean_vec_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_result[:,0], tsne_result[:,1], cmap='tab10')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
